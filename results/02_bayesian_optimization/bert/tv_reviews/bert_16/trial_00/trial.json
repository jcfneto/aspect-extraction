{"trial_id": "00", "hyperparameters": {"space": [{"class_name": "Choice", "config": {"name": "hidden_size", "default": 256, "conditions": [], "values": [256, 512, 768], "ordered": true}}, {"class_name": "Choice", "config": {"name": "num_hidden_layers", "default": 6, "conditions": [], "values": [6, 8, 10, 12], "ordered": true}}, {"class_name": "Choice", "config": {"name": "num_attention_heads", "default": 4, "conditions": [], "values": [4, 12, 16], "ordered": true}}, {"class_name": "Choice", "config": {"name": "intermediate_size", "default": 1024, "conditions": [], "values": [1024, 2048, 3072], "ordered": true}}, {"class_name": "Float", "config": {"name": "hidden_dropout_prob", "default": 0.1, "conditions": [], "min_value": 0.1, "max_value": 0.5, "step": 0.1, "sampling": "linear"}}, {"class_name": "Float", "config": {"name": "learning_rate", "default": 2e-05, "conditions": [], "min_value": 2e-05, "max_value": 0.001, "step": null, "sampling": "log"}}], "values": {"hidden_size": 256, "num_hidden_layers": 8, "num_attention_heads": 12, "intermediate_size": 3072, "hidden_dropout_prob": 0.30000000000000004, "learning_rate": 0.0002218887528384441}}, "metrics": {"metrics": {}}, "score": null, "best_step": 0, "status": "FAILED", "message": "Traceback (most recent call last):\n  File \"/usr/local/lib/python3.8/dist-packages/keras_tuner/engine/base_tuner.py\", line 270, in _try_run_and_update_trial\n    self._run_and_update_trial(trial, *fit_args, **fit_kwargs)\n  File \"/usr/local/lib/python3.8/dist-packages/keras_tuner/engine/base_tuner.py\", line 235, in _run_and_update_trial\n    results = self.run_trial(trial, *fit_args, **fit_kwargs)\n  File \"/usr/local/lib/python3.8/dist-packages/keras_tuner/engine/tuner.py\", line 287, in run_trial\n    obj_value = self._build_and_fit_model(trial, *args, **copied_kwargs)\n  File \"/usr/local/lib/python3.8/dist-packages/keras_tuner/engine/tuner.py\", line 213, in _build_and_fit_model\n    model = self._try_build(hp)\n  File \"/usr/local/lib/python3.8/dist-packages/keras_tuner/engine/tuner.py\", line 155, in _try_build\n    model = self._build_hypermodel(hp)\n  File \"/usr/local/lib/python3.8/dist-packages/keras_tuner/engine/tuner.py\", line 146, in _build_hypermodel\n    model = self.hypermodel.build(hp)\n  File \"/tmp/ipykernel_2205/3240867725.py\", line 12, in build_bert_model\n    model = TFBertForPreTraining(config)\n  File \"/usr/local/lib/python3.8/dist-packages/transformers/models/bert/modeling_tf_bert.py\", line 1171, in __init__\n    self.bert = TFBertMainLayer(config, name=\"bert\")\n  File \"/usr/local/lib/python3.8/dist-packages/transformers/modeling_tf_utils.py\", line 170, in wrapped_init\n    initializer(self, *args, **kwargs)\n  File \"/usr/local/lib/python3.8/dist-packages/transformers/models/bert/modeling_tf_bert.py\", line 727, in __init__\n    self.encoder = TFBertEncoder(config, name=\"encoder\")\n  File \"/usr/local/lib/python3.8/dist-packages/transformers/models/bert/modeling_tf_bert.py\", line 535, in __init__\n    self.layer = [TFBertLayer(config, name=f\"layer_._{i}\") for i in range(config.num_hidden_layers)]\n  File \"/usr/local/lib/python3.8/dist-packages/transformers/models/bert/modeling_tf_bert.py\", line 535, in <listcomp>\n    self.layer = [TFBertLayer(config, name=f\"layer_._{i}\") for i in range(config.num_hidden_layers)]\n  File \"/usr/local/lib/python3.8/dist-packages/transformers/models/bert/modeling_tf_bert.py\", line 449, in __init__\n    self.attention = TFBertAttention(config, name=\"attention\")\n  File \"/usr/local/lib/python3.8/dist-packages/transformers/models/bert/modeling_tf_bert.py\", line 371, in __init__\n    self.self_attention = TFBertSelfAttention(config, name=\"self\")\n  File \"/usr/local/lib/python3.8/dist-packages/transformers/models/bert/modeling_tf_bert.py\", line 237, in __init__\n    raise ValueError(\nValueError: The hidden size (256) is not a multiple of the number of attention heads (12)\n"}