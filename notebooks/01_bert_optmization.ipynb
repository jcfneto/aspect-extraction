{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import keras_tuner\n",
    "import nltk\n",
    "import random\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from transformers import (\n",
    "    AutoTokenizer, \n",
    "    BertConfig, \n",
    "    DataCollatorForLanguageModeling,\n",
    "    TFBertForPreTraining\n",
    ")\n",
    "\n",
    "nltk.download(\"punkt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BLOCK_SIZE = 512\n",
    "NSP_PROB = 0.50\n",
    "SHORT_SEQ_PROB = 0.1\n",
    "MAX_LENGTH = 512 \n",
    "\n",
    "MLM_PROB = 0.15\n",
    "\n",
    "MAX_EPOCHS = 10\n",
    "\n",
    "TRAIN_BATCH_SIZE = 8\n",
    "\n",
    "MODEL_CHECKPOINT = 'neuralmind/bert-base-portuguese-cased'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\n",
    "    \"text\", \n",
    "    data_files='../corpus/preprocessed/all_reviews_opt.txt'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_CHECKPOINT)\n",
    "max_num_tokens = BLOCK_SIZE - tokenizer.num_special_tokens_to_add(pair=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_CHECKPOINT)\n",
    "max_num_tokens = BLOCK_SIZE - tokenizer.num_special_tokens_to_add(pair=True)\n",
    "\n",
    "def prepare_train_features(examples):\n",
    "    \"\"\"Function to prepare features for NSP task\n",
    "\n",
    "    Arguments:\n",
    "      examples: A dictionary with 1 key (\"text\")\n",
    "        text: List of raw documents (str)\n",
    "    Returns:\n",
    "      examples:  A dictionary with 4 keys\n",
    "        input_ids: List of tokenized, concatnated, and batched\n",
    "          sentences from the individual raw documents (int)\n",
    "        token_type_ids: List of integers (0 or 1) corresponding\n",
    "          to: 0 for senetence no. 1 and padding, 1 for sentence\n",
    "          no. 2\n",
    "        attention_mask: List of integers (0 or 1) corresponding\n",
    "          to: 1 for non-padded tokens, 0 for padded\n",
    "        next_sentence_label: List of integers (0 or 1) corresponding\n",
    "          to: 1 if the second sentence actually follows the first,\n",
    "          0 if the senetence is sampled from somewhere else in the corpus\n",
    "    \"\"\"\n",
    "\n",
    "    # Remove un-wanted samples from the training set\n",
    "    examples[\"document\"] = [\n",
    "        d.strip() for d in examples[\"text\"] if len(d) > 0 and not d.startswith(\" =\")\n",
    "    ]\n",
    "    # Split the documents from the dataset into it's individual sentences\n",
    "    examples[\"sentences\"] = [\n",
    "        nltk.tokenize.sent_tokenize(document) for document in examples[\"document\"]\n",
    "    ]\n",
    "    # Convert the tokens into ids using the trained tokenizer\n",
    "    examples[\"tokenized_sentences\"] = [\n",
    "        [tokenizer.convert_tokens_to_ids(tokenizer.tokenize(sent)) for sent in doc]\n",
    "        for doc in examples[\"sentences\"]\n",
    "    ]\n",
    "\n",
    "    # Define the outputs\n",
    "    examples[\"input_ids\"] = []\n",
    "    examples[\"token_type_ids\"] = []\n",
    "    examples[\"attention_mask\"] = []\n",
    "    examples[\"next_sentence_label\"] = []\n",
    "\n",
    "    for doc_index, document in enumerate(examples[\"tokenized_sentences\"]):\n",
    "\n",
    "        current_chunk = []  # a buffer stored current working segments\n",
    "        current_length = 0\n",
    "        i = 0\n",
    "\n",
    "        # We *usually* want to fill up the entire sequence since we are padding\n",
    "        # to `block_size` anyways, so short sequences are generally wasted\n",
    "        # computation. However, we *sometimes*\n",
    "        # (i.e., short_seq_prob == 0.1 == 10% of the time) want to use shorter\n",
    "        # sequences to minimize the mismatch between pretraining and fine-tuning.\n",
    "        # The `target_seq_length` is just a rough target however, whereas\n",
    "        # `block_size` is a hard limit.\n",
    "        target_seq_length = max_num_tokens\n",
    "\n",
    "        if random.random() < SHORT_SEQ_PROB:\n",
    "            target_seq_length = random.randint(2, max_num_tokens)\n",
    "\n",
    "        while i < len(document):\n",
    "            segment = document[i]\n",
    "            current_chunk.append(segment)\n",
    "            current_length += len(segment)\n",
    "            if i == len(document) - 1 or current_length >= target_seq_length:\n",
    "                if current_chunk:\n",
    "                    # `a_end` is how many segments from `current_chunk` go into the `A`\n",
    "                    # (first) sentence.\n",
    "                    a_end = 1\n",
    "                    if len(current_chunk) >= 2:\n",
    "                        a_end = random.randint(1, len(current_chunk) - 1)\n",
    "\n",
    "                    tokens_a = []\n",
    "                    for j in range(a_end):\n",
    "                        tokens_a.extend(current_chunk[j])\n",
    "\n",
    "                    tokens_b = []\n",
    "\n",
    "                    if len(current_chunk) == 1 or random.random() < NSP_PROB:\n",
    "                        is_random_next = True\n",
    "                        target_b_length = target_seq_length - len(tokens_a)\n",
    "\n",
    "                        # This should rarely go for more than one iteration for large\n",
    "                        # corpora. However, just to be careful, we try to make sure that\n",
    "                        # the random document is not the same as the document\n",
    "                        # we're processing.\n",
    "                        for _ in range(10):\n",
    "                            random_document_index = random.randint(\n",
    "                                0, len(examples[\"tokenized_sentences\"]) - 1\n",
    "                            )\n",
    "                            if random_document_index != doc_index:\n",
    "                                break\n",
    "\n",
    "                        random_document = examples[\"tokenized_sentences\"][\n",
    "                            random_document_index\n",
    "                        ]\n",
    "                        random_start = random.randint(0, len(random_document) - 1)\n",
    "                        for j in range(random_start, len(random_document)):\n",
    "                            tokens_b.extend(random_document[j])\n",
    "                            if len(tokens_b) >= target_b_length:\n",
    "                                break\n",
    "                        # We didn't actually use these segments so we \"put them back\" so\n",
    "                        # they don't go to waste.\n",
    "                        num_unused_segments = len(current_chunk) - a_end\n",
    "                        i -= num_unused_segments\n",
    "                    else:\n",
    "                        is_random_next = False\n",
    "                        for j in range(a_end, len(current_chunk)):\n",
    "                            tokens_b.extend(current_chunk[j])\n",
    "\n",
    "                    input_ids = tokenizer.build_inputs_with_special_tokens(\n",
    "                        tokens_a, tokens_b\n",
    "                    )\n",
    "                    # add token type ids, 0 for sentence a, 1 for sentence b\n",
    "                    token_type_ids = tokenizer.create_token_type_ids_from_sequences(\n",
    "                        tokens_a, tokens_b\n",
    "                    )\n",
    "\n",
    "                    padded = tokenizer.pad(\n",
    "                        {\"input_ids\": input_ids, \"token_type_ids\": token_type_ids},\n",
    "                        padding=\"max_length\",\n",
    "                        max_length=MAX_LENGTH,\n",
    "                    )\n",
    "\n",
    "                    examples[\"input_ids\"].append(padded[\"input_ids\"])\n",
    "                    examples[\"token_type_ids\"].append(padded[\"token_type_ids\"])\n",
    "                    examples[\"attention_mask\"].append(padded[\"attention_mask\"])\n",
    "                    examples[\"next_sentence_label\"].append(1 if is_random_next else 0)\n",
    "                    current_chunk = []\n",
    "                    current_length = 0\n",
    "            i += 1\n",
    "\n",
    "    # We delete all the un-necessary columns from our dataset\n",
    "    del examples[\"document\"]\n",
    "    del examples[\"sentences\"]\n",
    "    del examples[\"text\"]\n",
    "    del examples[\"tokenized_sentences\"]\n",
    "\n",
    "    return examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_dataset = dataset.map(\n",
    "    prepare_train_features, \n",
    "    batched=True, \n",
    "    remove_columns=[\"text\"], \n",
    "    num_proc=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "collater = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer, \n",
    "    mlm=True, \n",
    "    mlm_probability=MLM_PROB, \n",
    "    return_tensors=\"tf\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = tokenized_dataset[\"train\"].to_tf_dataset(\n",
    "    columns=[\"input_ids\", \"token_type_ids\", \"attention_mask\"],\n",
    "    label_cols=[\"labels\", \"next_sentence_label\"],\n",
    "    batch_size=TRAIN_BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    collate_fn=collater,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_bert_model(hp):\n",
    "    \"\"\"Function to build a BERT model. `hp` is an instance of `keras_tuner.HyperParameters`\"\"\"\n",
    "\n",
    "    config = BertConfig(\n",
    "        vocab_size=tokenizer.vocab_size,  \n",
    "        hidden_size=hp.Choice('hidden_size', values=[256, 512, 768]),  \n",
    "        num_hidden_layers=hp.Choice(\n",
    "            'num_hidden_layers', values=[6, 8, 10, 12]\n",
    "        ),\n",
    "        num_attention_heads=hp.Choice(\n",
    "            'num_attention_heads', values=[4, 12, 16]\n",
    "        ),\n",
    "        intermediate_size=hp.Choice(\n",
    "            'intermediate_size', values=[1024, 2048, 3072]\n",
    "        ),\n",
    "        hidden_dropout_prob=hp.Float(\n",
    "            'hidden_dropout_prob', min_value=0.1, max_value=0.5, step=0.1\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    model = TFBertForPreTraining(config)\n",
    "    optimizer = keras.optimizers.experimental.AdamW(\n",
    "        learning_rate=hp.Float(\n",
    "            'learning_rate', min_value=2e-5, max_value=1e-3, sampling='log'\n",
    "        )\n",
    "    )\n",
    "    model.compile(optimizer=optimizer)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStoppingPercent(tf.keras.callbacks.Callback):\n",
    "    \"\"\"Callback to stop training when the loss stops decreasing by a certain percentage.\"\"\"\n",
    "    def __init__(\n",
    "            self, \n",
    "            patience=3, \n",
    "            min_percent_change=0.01\n",
    "        ):\n",
    "        super(EarlyStoppingPercent, self).__init__()\n",
    "        self.patience = patience\n",
    "        self.min_percent_change = min_percent_change\n",
    "        self.best_loss = float(\"inf\")\n",
    "        self.wait = 0\n",
    "        self.best_weights = None\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        current_loss = logs.get(\"loss\")\n",
    "\n",
    "        if self.best_loss == float(\"inf\"):\n",
    "            self.best_loss = current_loss\n",
    "            return\n",
    "\n",
    "        percent_change = (self.best_loss - current_loss) / self.best_loss\n",
    "\n",
    "        if percent_change > self.min_percent_change:\n",
    "            self.best_loss = current_loss\n",
    "            self.wait = 0\n",
    "        else:\n",
    "            self.wait += 1\n",
    "            if self.wait >= self.patience:\n",
    "                self.model.stop_training = True\n",
    "                print(f\" - Epoch {epoch + 1}: Early stopping triggered.\")\n",
    "\n",
    "        print(f' - Best value is {self.best_loss}.')\n",
    "\n",
    "early_stopping = EarlyStoppingPercent(patience=3, min_percent_change=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner = keras_tuner.BayesianOptimization(\n",
    "    hypermodel=build_bert_model,\n",
    "    objective='loss',\n",
    "    max_trials=20,\n",
    "    overwrite=True,\n",
    "    directory=f'/aspect_extraction/notebooks/results/bo_search_{TRAIN_BATCH_SIZE}',\n",
    "    project_name=f'/aspect_extraction/notebooks/results/bert_mod_{TRAIN_BATCH_SIZE}'\n",
    ")\n",
    "\n",
    "tuner.search(\n",
    "    train,\n",
    "    epochs=10,\n",
    "    callbacks=[early_stopping]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "params = (\n",
    "    'hidden_size',\n",
    "    'num_hidden_layers',\n",
    "    'num_attention_heads',\n",
    "    'intermediate_size',\n",
    "    'hidden_dropout_prob',\n",
    "    'learning_rate',\n",
    ")\n",
    "\n",
    "for param in params:\n",
    "    print(f'The best value for {param} is {best_hps.get(param)}.')"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
