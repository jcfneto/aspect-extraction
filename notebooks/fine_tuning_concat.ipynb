{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import spacy\n",
    "import optuna\n",
    "import evaluate\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "from ast import literal_eval\n",
    "from datasets import Dataset, DatasetDict\n",
    "from src import utils\n",
    "\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras.losses import SparseCategoricalCrossentropy\n",
    "from tensorflow.keras.callbacks import Callback\n",
    "from tensorflow.keras.optimizers.experimental import AdamW\n",
    "\n",
    "from transformers import (\n",
    "    TFBertModel,\n",
    "    DataCollatorForTokenClassification\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parÃ¢metros glabais\n",
    "data_path = '../datasets/stratified/tv.csv'\n",
    "general_domain_checkpoint = 'neuralmind/bert-base-portuguese-cased'\n",
    "specific_domain_checkpoint = 'jcfneto/bert-tv-portuguese'\n",
    "max_length = 512\n",
    "num_classes = 3\n",
    "batch_size = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(data_path)\n",
    "\n",
    "cols = ['tokens', 'aspect_tags']\n",
    "for col in cols:\n",
    "    data[col] = data[col].apply(literal_eval)\n",
    "    \n",
    "indexs = []\n",
    "nlp = spacy.load('pt_core_news_sm')\n",
    "\n",
    "postags = []\n",
    "for i, tokens in enumerate(data.tokens):\n",
    "    sentence = ' '.join(tokens)\n",
    "    doc = nlp(sentence)\n",
    "    pos = [token.pos_ for token in doc]\n",
    "    postags.append(pos)\n",
    "    if len(pos) != len(tokens):\n",
    "        indexs.append(i)\n",
    "\n",
    "data['pos'] = postags\n",
    "\n",
    "print(data.shape)\n",
    "for idx in indexs:\n",
    "    data = data.drop(index=idx)\n",
    "print(data.shape)\n",
    "\n",
    "# tag mapping\n",
    "id2label = {0: 'O', 1: 'B-ASP', 2: 'I-ASP'}\n",
    "label2id = {v: k for k, v in id2label.items()}\n",
    "label_names = ['O', 'B-ASP', 'I-ASP']\n",
    "\n",
    "pos = data.pos.values\n",
    "pos = set([tag for p in pos for tag in p])\n",
    "\n",
    "ids = np.linspace(-1, 1, 16)\n",
    "\n",
    "pos2id = {tag: i for i, tag in zip(ids, pos)}\n",
    "id2pos = {i: tag for tag, i in pos2id.items()}\n",
    "\n",
    "def tag2id(example):\n",
    "    return [label2id[tag] for tag in example]\n",
    "\n",
    "def postag2id(example):\n",
    "    return [pos2id[tag] for tag in example]\n",
    "\n",
    "data.aspect_tags = data.aspect_tags.apply(lambda x: tag2id(x))\n",
    "data.pos = data.pos.apply(lambda x: postag2id(x))\n",
    "\n",
    "tokenizer = utils.build_tokenizer(general_domain_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def align_labels_with_tokens(labels: list, word_ids: list):\n",
    "    \"\"\"Aligns labels with subword tokens.\n",
    "\n",
    "    Args:\n",
    "        labels: List with labels.\n",
    "        word_ids: Word index.\n",
    "\n",
    "    Returns:\n",
    "        List with labels aligned.\n",
    "    \"\"\"\n",
    "    new_labels = []\n",
    "    current_word = None\n",
    "    for word_id in word_ids:\n",
    "        if word_id != current_word:\n",
    "            current_word = word_id\n",
    "            label = -100 if word_id is None else labels[word_id]\n",
    "            new_labels.append(label)\n",
    "        elif word_id is None:\n",
    "            new_labels.append(-100)\n",
    "        else:\n",
    "            label = labels[word_id]\n",
    "            if label % 2 == 1:\n",
    "                label += 1\n",
    "            new_labels.append(label)\n",
    "    return new_labels\n",
    "\n",
    "\n",
    "def tokenize_and_align_labels(examples: dict) -> Dataset:\n",
    "    \"\"\"Tokenize and align labels with subword tokens.\n",
    "\n",
    "    Args:\n",
    "        examples: Pre-token.\n",
    "\n",
    "    Returns:\n",
    "        Tokens with labels.\n",
    "    \"\"\"\n",
    "    tokenized_inputs = tokenizer(\n",
    "        examples['tokens'],\n",
    "        padding='max_length',\n",
    "        max_length=max_length,\n",
    "        truncation=True,\n",
    "        is_split_into_words=True,\n",
    "    )\n",
    "    all_aspect_labels = examples['aspect_tags']\n",
    "    new_aspect_labels = []\n",
    "    new_pos_labels = []\n",
    "    for i, labels in enumerate(all_aspect_labels):\n",
    "        word_ids = tokenized_inputs.word_ids(i)\n",
    "        new_aspect_labels.append(align_labels_with_tokens(labels, word_ids) )\n",
    "    tokenized_inputs['aspect_labels'] = new_aspect_labels\n",
    "    return tokenized_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = ['tokens', 'aspect_tags', 'pos']\n",
    "\n",
    "data_ds = DatasetDict({\n",
    "    f'fold_{fold}': Dataset.from_pandas(\n",
    "    data[data.fold == fold][cols], \n",
    "    preserve_index=False\n",
    "    )\n",
    "        for fold in data.fold.unique()\n",
    "})\n",
    "\n",
    "\n",
    "for fold in data_ds:\n",
    "    data_ds[fold] = data_ds[fold].map(\n",
    "        tokenize_and_align_labels,\n",
    "        batched=True,\n",
    "        remove_columns=data_ds[fold].column_names\n",
    "    )\n",
    "\n",
    "\n",
    "# params to data collator\n",
    "columns = data_ds['fold_1'].column_names\n",
    "data_collator = DataCollatorForTokenClassification(\n",
    "    tokenizer=tokenizer,\n",
    "    return_tensors='tf'\n",
    ")\n",
    "\n",
    "\n",
    "# data collator\n",
    "for fold in data_ds:\n",
    "    data_ds[fold] = data_ds[fold].to_tf_dataset(\n",
    "        columns=columns,\n",
    "        collate_fn=data_collator,\n",
    "        shuffle=False,\n",
    "        batch_size=batch_size\n",
    "    )\n",
    "\n",
    "folds = list(data_ds.keys())\n",
    "train = data_ds[folds[0]]\n",
    "\n",
    "for fold in folds[1:8]:\n",
    "    train = train.concatenate(data_ds[fold])\n",
    "\n",
    "test = data_ds[folds[8]]\n",
    "validation = data_ds[folds[9]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class F1ScoreCallback(Callback):\n",
    "    def __init__(self, validation_data, label_names):\n",
    "        super(F1ScoreCallback, self).__init__()\n",
    "        self.validation_data = validation_data\n",
    "        self.label_names = label_names\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        logs = logs or {}\n",
    "        f1_score = val(self.model, self.validation_data, self.label_names)\n",
    "        logs[\"val_f1_score\"] = f1_score\n",
    "\n",
    "\n",
    "def val(model, test_data, label_names):\n",
    "    all_labels = []\n",
    "    all_predictions = []\n",
    "    metric = evaluate.load(\"seqeval\")\n",
    "    for batch in test_data:\n",
    "        logits = model.predict_on_batch(batch)\n",
    "        labels = batch[\"aspect_labels\"]\n",
    "        predictions = tf.argmax(logits, axis=-1)\n",
    "        for prediction, label in zip(predictions, labels):\n",
    "            for predicted_idx, label_idx in zip(prediction, label):\n",
    "                if label_idx == -100:\n",
    "                    continue\n",
    "                all_predictions.append(label_names[predicted_idx])\n",
    "                all_labels.append(label_names[label_idx])\n",
    "    results = metric.compute(\n",
    "        predictions=[all_predictions],\n",
    "        references=[all_labels]\n",
    "    )\n",
    "\n",
    "    return results['overall_f1']\n",
    "\n",
    "\n",
    "def val2(model, test_data, label_names):\n",
    "    all_labels = []\n",
    "    all_predictions = []\n",
    "    metric = evaluate.load(\"seqeval\")\n",
    "    for batch in test_data:\n",
    "        logits = model.predict_on_batch(batch)\n",
    "        labels = batch[\"aspect_labels\"]\n",
    "        predictions = tf.argmax(logits, axis=-1)\n",
    "        for prediction, label in zip(predictions, labels):\n",
    "            for predicted_idx, label_idx in zip(prediction, label):\n",
    "                if label_idx == -100:\n",
    "                    continue\n",
    "                all_predictions.append(label_names[predicted_idx])\n",
    "                all_labels.append(label_names[label_idx])\n",
    "    results = metric.compute(\n",
    "        predictions=[all_predictions],\n",
    "        references=[all_labels]\n",
    "    )\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "def expand_data(data):\n",
    "    if isinstance(data, dict):\n",
    "        return data\n",
    "    else:\n",
    "        return {k: v for k, v in zip(data.keys(), data)}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Concat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AEConcatPosTag(Model):\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        bert_general_domain,\n",
    "        bert_specific_domain,\n",
    "        dropout_rate,\n",
    "        n_classes=3,\n",
    "        train_bert_layer=False,\n",
    "    ):\n",
    "        super(AEConcatPosTag, self).__init__()\n",
    "        self.bert_general_domain = bert_general_domain\n",
    "        self.bert_specific_domain = bert_specific_domain\n",
    "\n",
    "        self.bert_general_domain.trainable = train_bert_layer\n",
    "        self.bert_specific_domain.trainable = train_bert_layer\n",
    "\n",
    "        self.concat = layers.Concatenate()\n",
    "\n",
    "        self.dropout = layers.Dropout(dropout_rate)\n",
    "        \n",
    "        self.classifier = layers.Dense(\n",
    "            n_classes, activation='softmax'\n",
    "        )\n",
    "\n",
    "    def call(self, inputs, training=False):\n",
    "        input_ids = inputs['input_ids']\n",
    "        token_type_ids = inputs['token_type_ids']\n",
    "        attention_mask = inputs['attention_mask']\n",
    "\n",
    "        output_general_domain = self.bert_general_domain([\n",
    "            input_ids, token_type_ids, attention_mask\n",
    "        ])\n",
    "\n",
    "        output_specific_domain = self.bert_specific_domain([\n",
    "            input_ids, token_type_ids, attention_mask\n",
    "        ])\n",
    "\n",
    "        concatened = tf.concat([\n",
    "            output_general_domain['last_hidden_state'],\n",
    "            output_specific_domain['last_hidden_state'],\n",
    "        ], axis=-1)\n",
    "\n",
    "        logits = self.classifier(self.dropout(concatened))\n",
    "\n",
    "        return logits\n",
    "    \n",
    "    def train_step(self, data):\n",
    "        data = expand_data(data)\n",
    "        inputs = {\n",
    "            k: data[k] for k in (\n",
    "                'input_ids', \n",
    "                'token_type_ids', \n",
    "                'attention_mask',\n",
    "            )\n",
    "        }\n",
    "        labels = data['aspect_labels']\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            logits = self(inputs, training=True)\n",
    "            loss = self.compiled_loss(\n",
    "                labels, \n",
    "                logits, \n",
    "                regularization_losses=self.losses\n",
    "            )\n",
    "\n",
    "        trainable_vars = self.trainable_variables\n",
    "        gradients = tape.gradient(loss, trainable_vars)\n",
    "        self.optimizer.apply_gradients(zip(gradients, trainable_vars))\n",
    "\n",
    "        self.compiled_metrics.update_state(labels, logits)\n",
    "        return {m.name: m.result() for m in self.metrics}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AEConcatPosTag(\n",
    "    bert_general_domain=TFBertModel.from_pretrained(general_domain_checkpoint),\n",
    "    bert_specific_domain=TFBertModel.from_pretrained(specific_domain_checkpoint),\n",
    "    dropout_rate=0.3,\n",
    "    train_bert_layer=True\n",
    ")\n",
    "\n",
    "optimizer = AdamW(learning_rate=2.0863364780543777e-05)\n",
    "loss_fn = SparseCategoricalCrossentropy(from_logits=False, ignore_class=-100)\n",
    "f1_score_callback = F1ScoreCallback(test, label_names)\n",
    "\n",
    "model.compile(\n",
    "    optimizer=optimizer, \n",
    "    loss=loss_fn\n",
    ")\n",
    "\n",
    "history = model.fit(\n",
    "    train,\n",
    "    epochs=10,\n",
    "    callbacks=[f1_score_callback]\n",
    ")\n",
    "\n",
    "val_f1_score = max(history.history['val_f1_score'])\n",
    "best_epoch = history.history['val_f1_score'].index(val_f1_score)\n",
    "\n",
    "print(f'Best f1 score is {val_f1_score} on epoch {best_epoch}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Concat + FFN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AEConcatPosTag(Model):\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        bert_general_domain,\n",
    "        bert_specific_domain,\n",
    "        dropout_rate,\n",
    "        first_layer_units,\n",
    "        second_layer_units=None,\n",
    "        third_layer_units=None,\n",
    "        n_classes=3,\n",
    "        train_bert_layer=False,\n",
    "    ):\n",
    "        super(AEConcatPosTag, self).__init__()\n",
    "        self.bert_general_domain = bert_general_domain\n",
    "        self.bert_specific_domain = bert_specific_domain\n",
    "\n",
    "        self.bert_general_domain.trainable = train_bert_layer\n",
    "        self.bert_specific_domain.trainable = train_bert_layer\n",
    "\n",
    "        self.concat = layers.Concatenate()\n",
    "        self.dropout = layers.Dropout(dropout_rate)\n",
    "\n",
    "        self.dense1 = layers.Dense(first_layer_units, activation='relu')\n",
    "        self.dropout1 = layers.Dropout(dropout_rate)\n",
    "\n",
    "        self.second_layer_units = second_layer_units\n",
    "        if second_layer_units:\n",
    "            self.dense2 = layers.Dense(second_layer_units, activation='relu')\n",
    "            self.dropout2 = layers.Dropout(dropout_rate)\n",
    "\n",
    "        self.third_layer_units = third_layer_units\n",
    "        if third_layer_units:\n",
    "            self.dense3 = layers.Dense(third_layer_units, activation='relu')\n",
    "            self.dropout3 = layers.Dropout(dropout_rate)\n",
    "\n",
    "        self.classifier = layers.Dense(\n",
    "            n_classes, activation='softmax'\n",
    "        )\n",
    "\n",
    "    def call(self, inputs, training=False):\n",
    "        input_ids = inputs['input_ids']\n",
    "        token_type_ids = inputs['token_type_ids']\n",
    "        attention_mask = inputs['attention_mask']\n",
    "\n",
    "        output_general_domain = self.bert_general_domain([\n",
    "            input_ids, token_type_ids, attention_mask\n",
    "        ])\n",
    "\n",
    "        output_specific_domain = self.bert_specific_domain([\n",
    "            input_ids, token_type_ids, attention_mask\n",
    "        ])\n",
    "\n",
    "        concatened = tf.concat([\n",
    "            output_general_domain['last_hidden_state'],\n",
    "            output_specific_domain['last_hidden_state'],\n",
    "        ], axis=-1)\n",
    "\n",
    "        concatened = self.dropout(concatened)\n",
    "\n",
    "        logits = self.dense1(concatened)\n",
    "        logits = self.dropout1(logits)\n",
    "\n",
    "        if self.second_layer_units:\n",
    "            logits = self.dense2(logits)\n",
    "            logits = self.dropout2(logits)\n",
    "\n",
    "        if self.third_layer_units:\n",
    "            logits = self.dense3(logits)\n",
    "            logits = self.dropout3(logits)         \n",
    "\n",
    "        logits = self.classifier(logits)\n",
    "\n",
    "        return logits\n",
    "    \n",
    "    def train_step(self, data):\n",
    "        data = expand_data(data)\n",
    "        inputs = {\n",
    "            k: data[k] for k in (\n",
    "                'input_ids', \n",
    "                'token_type_ids', \n",
    "                'attention_mask',\n",
    "            )\n",
    "        }\n",
    "        labels = data['aspect_labels']\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            logits = self(inputs, training=True)\n",
    "            loss = self.compiled_loss(\n",
    "                labels, \n",
    "                logits, \n",
    "                regularization_losses=self.losses\n",
    "            )\n",
    "\n",
    "        trainable_vars = self.trainable_variables\n",
    "        gradients = tape.gradient(loss, trainable_vars)\n",
    "        self.optimizer.apply_gradients(zip(gradients, trainable_vars))\n",
    "\n",
    "        self.compiled_metrics.update_state(labels, logits)\n",
    "        return {m.name: m.result() for m in self.metrics}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AEConcatPosTag(\n",
    "    bert_general_domain=TFBertModel.from_pretrained(general_domain_checkpoint),\n",
    "    bert_specific_domain=TFBertModel.from_pretrained(specific_domain_checkpoint),\n",
    "    dropout_rate=0.30000000000000004,\n",
    "    first_layer_units=768,\n",
    "    second_layer_units=1536,\n",
    "    third_layer_units=512,\n",
    "    n_classes=3,\n",
    "    train_bert_layer=True\n",
    ")\n",
    "\n",
    "optimizer = AdamW(learning_rate=2.006300676202351e-05)\n",
    "loss_fn = SparseCategoricalCrossentropy(from_logits=False, ignore_class=-100)\n",
    "f1_score_callback = F1ScoreCallback(test, label_names)\n",
    "\n",
    "model.compile(\n",
    "    optimizer=optimizer, \n",
    "    loss=loss_fn\n",
    ")\n",
    "\n",
    "history = model.fit(\n",
    "    train,\n",
    "    epochs=23,\n",
    "    callbacks=[f1_score_callback]\n",
    ")\n",
    "\n",
    "val_f1_score = max(history.history['val_f1_score'])\n",
    "best_epoch = history.history['val_f1_score'].index(val_f1_score)\n",
    "\n",
    "print(f'Best f1 score is {val_f1_score} on epoch {best_epoch}')"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
